# -*- coding: utf-8 -*-
"""
Created on Sun Jun 11 12:32:04 2023

@author: mariasanmartin
"""


#Carga de librerias
import pandas as pd
import seaborn as sn

import matplotlib.pyplot as plt
import os
import numpy as np

import sklearn
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split

from sklearn.neural_network import MLPRegressor

from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import r2_score
from sklearn.metrics import classification_report,confusion_matrix

#############################################################################################
#         CARGA Y PREPROCESAMIENTO DE LOS DATOS
#############################################################################################


df = pd.read_csv('german_credit_data.csv')





# Función que calcula los missing values por columna
def missing_values_table(df):
        # Número total de missing values
        mis_val = df.isnull().sum()
        
        # Porcentaje de missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Dataframe con los resultados
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Renombrar las columnas
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Ordenar el dataframe por porcentaje de missing values
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Informar de los resultados
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
       
        return mis_val_table_ren_columns


# Información de los missing values del dataframe
missing_values = missing_values_table(df)
missing_values.head(20)


print(df.dtypes.value_counts())
# Eliminar las columnas con más de un 30% de missing values
for columna in missing_values.index:
    if missing_values['% of Total Values'][columna] > 30:
        df = df.drop(columns = columna)

print(df.dtypes.value_counts())

df["Saving accounts"].fillna(df['Saving accounts'].mode()[0], inplace=True)

# Matriz de correlación
corr_matrix = df.corr()
sn.heatmap(corr_matrix, annot=True)
plt.show()

correlaciones = corr_matrix.abs().unstack().reset_index()
correlaciones = correlaciones[~(correlaciones['level_0'] == correlaciones['level_1'])]


# Eliminar variables con correlación mayor a 0.95
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]

df = df.drop(to_drop, axis=1)


# Asegurar que todos los datos estén bien detectados

print(df.dtypes)


# One-hot encoding de variables ctegoricas

categorical_columns = df.select_dtypes(include=['object']).columns.tolist() 

# Convertir columnas catgoricas a  dummy variables
dummy_df = pd.get_dummies(df[categorical_columns], drop_first=True)


df = pd.concat([df.drop(columns=categorical_columns), dummy_df], axis=1)




#############################################################################################
#         ENTRENAMIENTO DEL MODELO
#############################################################################################

# Eliminamos la primera variable que es el ID del solicitante y la variable target
X = df.drop(columns=df.columns[[0, -1]],)

#Se escalan los datos
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
y = df[df.columns[-1]]

# Separacion del train y el test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)

mlp = MLPClassifier(hidden_layer_sizes=( 20,10,8), activation='relu', solver='adam', max_iter=100, random_state=123)
mlp.fit(X_train,y_train)

predict_train = mlp.predict(X_train)
predict_test = mlp.predict(X_test)


print(confusion_matrix(y_train,predict_train))
print(classification_report(y_train,predict_train))

nn_cm =confusion_matrix(y_train,predict_train)

trainedmodel = X_train.assign(PredictionTrain=predict_train)

accuracy = (nn_cm[0,0] + nn_cm[1,1]) / np.sum(nn_cm)


# Es peor clsificar a uno como bueno y que lugo sea malo a que se le clasifique como malo y uego seea bueno
nn_cm_test =confusion_matrix(y_test,predict_test)

accuracy_test = (nn_cm_test[0,0] + nn_cm_test[1,1]) / np.sum(nn_cm_test)


threshold = 0.6
y_pred = (mlp.predict_proba(X_train)[:, 1] > threshold).astype('float')
nn_cm_th_tr = confusion_matrix(y_train, y_pred)

accuracy_th_tr = (nn_cm_th_tr[0,0] + nn_cm_th_tr[1,1]) / np.sum(nn_cm_th_tr)

threshold = 0.6
y_pred = (mlp.predict_proba(X_test)[:, 1] > threshold).astype('float')
nn_cm_th_test = confusion_matrix(y_test, y_pred)

accuracy_th_test = (nn_cm_th_test[0,0] + nn_cm_th_test[1,1]) / np.sum(nn_cm_th_test)

#############################################################################################
#         ICE
#############################################################################################
from time import time
from sklearn.inspection import PartialDependenceDisplay

print("Computing partial dependence plots and individual conditional expectation...")
tic = time()
_, ax = plt.subplots(ncols=2, nrows=3, figsize=(6, 4), sharey=True, constrained_layout=True)

features_info = {
    "features": ["Age", 'Job', 'Credit amount', 'Duration', 'Sex_male', 'Housing_own'],
    "kind": "both",
}


common_params = {
    "subsample": 50,
    "n_jobs": 2,
    "grid_resolution": 20,
    "random_state": 0,
}

display = PartialDependenceDisplay.from_estimator(
    mlp,
    X_train,
    **features_info,
    ax=ax,
    **common_params,
)
print(f"done in {time() - tic:.3f}s")
_ = display.figure_.suptitle("ICE and PDP representations", fontsize=16)


plt.show()
#############################################################################################
#         LIME
#############################################################################################

import lime
from lime import lime_tabular

explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=[0, 1],
    mode='classification'
)

exp = explainer.explain_instance(
    data_row=X_test.iloc[1], 
    predict_fn=mlp.predict_proba
)

exp.show_in_notebook(show_table=True)


exp = explainer.explain_instance(
    data_row=X_test.iloc[2], 
    predict_fn=mlp.predict_proba
)


exp.show_in_notebook(show_table=True)

exp = explainer.explain_instance(
    data_row=X_test.iloc[3], 
    predict_fn=mlp.predict_proba
)

exp.show_in_notebook(show_table=True)
#the middle section returns 5 most important features. For the binary classification task, it would be in 2 colors orange/blue. Attributes in orange support class 1 and those in blue support class 0. Sex_le ≤0 supports class 1. Float point numbers on the horizontal bars represent the relative importance of these features.
exp.as_list()


#############################################################################################
#         SHAP
#############################################################################################



import shap

explainer = shap.KernelExplainer(mlp.predict_proba, X_train)
shap_values = explainer.shap_values(X_test.iloc[0,:])
shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0,:])




#############################################################################################
#         Counterfactual explanations
#############################################################################################




#############################################################################################
#         Anchors
#############################################################################################



